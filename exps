exp 2
import numpy as np

# Generate sample data

np.random.seed(42)

X = np.random.rand(100, 1)

y = 3 * X + 2 + 0.1 * np.random.randn(100, 1)  # y = 3X + 2 + noise

# Initialize parameters

learning_rate = 0.01

epochs = 1000

weights = np.random.rand(2, 1)  # Slope and intercept

# Print initial weights

print("Initial weights:", weights)

# Gradient Descent

for epoch in range(epochs):

    X_b = np.c_[np.ones((len(X), 1)), X]  # Add bias term to X

    y_pred = X_b.dot(weights)

    error = y_pred - y

    gradient = 2 * X_b.T.dot(error) / len(X_b)

    weights -= learning_rate * gradient

    

    mse = np.mean(error ** 2)

    if epoch % 100 == 0:

        print(f"Epoch {epoch}, MSE: {mse:.4f}")

print("Final weights:", weights)





exp 3
mport numpy as np

import matplotlib.pyplot as plt

# Generate sample data

np.random.seed(42)

X = np.random.rand(100, 1)

y = 3 * X + 2 + 0.1 * np.random.randn(100, 1)  # y = 3X + 2 + noise

# Calculate mean of X and y

x_mean = np.mean(X)

y_mean = np.mean(y)

# Calculate slope (m) and intercept (b) using closed-form formulas

numerator = np.sum((X - x_mean) * (y - y_mean))

denominator = np.sum((X - x_mean) ** 2)

slope = numerator / denominator

intercept = y_mean - slope * x_mean

print("The slope and Intercept are:-")

print("Slope:", slope)

print("Intercept:", intercept)

plt.scatter(X, y, label='Data Points')

plt.plot(X, slope * X + intercept, color='red', label='Regression Line')

plt.xlabel('X')

plt.ylabel('y')

plt.title('Simple Linear Regression')

plt.legend()

plt.show()



exp 4
import numpy as np

import matplotlib.pyplot as plt

class LogisticRegression:

    def __init__(self, learning_rate=0.01, num_iterations=1000):

        self.learning_rate = learning_rate

        self.num_iterations = num_iterations

        self.weights = None

        self.bias = None

        

    def sigmoid(self, z):

        return 1 / (1 + np.exp(-z))

    

    def initialize_params(self, num_features):

        self.weights = np.zeros(num_features)

        self.bias = 0

        

    def fit(self, X, y):

        num_samples, num_features = X.shape

        self.initialize_params(num_features)

        

        # Gradient descent

        for _ in range(self.num_iterations):

            linear_model = np.dot(X, self.weights) + self.bias

            predictions = self.sigmoid(linear_model)

            

            # Calculate gradients

            dw = (1/num_samples) * np.dot(X.T, (predictions - y))

            db = (1/num_samples) * np.sum(predictions - y)

            

            # Update parameters

            self.weights -= self.learning_rate * dw

            self.bias -= self.learning_rate * db

            

    def predict(self, X):

        linear_model = np.dot(X, self.weights) + self.bias

        predictions = self.sigmoid(linear_model)

        y_predicted = [1 if p > 0.5 else 0 for p in predictions]

        return y_predicted

# Example usage

if __name__ == "__main__":

    # Sample data

    X = np.array([[2.5, 3.0], [1.5, 2.2], [3.5, 2.5], [3.0, 3.5],

                  [1.0, 1.0], [3.5, 4.0], [4.0, 2.0], [2.7, 2.7]])

    y = np.array([0, 0, 1, 1, 0, 1, 1, 0])

    

    model = LogisticRegression(learning_rate=0.01, num_iterations=1000)

    model.fit(X, y)

    

    # Predictions

    new_samples = np.array([[2.0, 2.5], [3.5, 4.0]])

    predictions = model.predict(new_samples)

    print(predictions)

    

    # Visualize decision boundary

    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)

    

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1

    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),

                         np.arange(y_min, y_max, 0.01))

    

    Z = np.array(model.predict(np.c_[xx.ravel(), yy.ravel()]))

    Z = Z.reshape(xx.shape)

    

    plt.contourf(xx, yy, Z, alpha=0.3)

    plt.xlabel('Feature 1')

    plt.ylabel('Feature 2')

    plt.title('Logistic Regression Decision Boundary')

    plt.show()

